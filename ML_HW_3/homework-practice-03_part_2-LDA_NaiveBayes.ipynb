{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef3a7a1",
   "metadata": {},
   "source": [
    "## Гауссово распределение, линейный дискриминантный анализ и наивный байес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f651706",
   "metadata": {},
   "source": [
    "### 1. Многомерное гауссово распределение\n",
    "Создайте случайную выборку данных с двумерным гауссовым распределением. Для этого сперва сделайте два вектора из двух одномерных распределений с разными стандартными отклонениями $\\sigma_1$ и $\\sigma_2$, как показано ниже:  \n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "M = 200\n",
    "sigma1 = 0.1\n",
    "sigma2 = 0.5\n",
    "x1 = np.random.randn(M, 1) * sigma1\n",
    "x2 = np.random.randn(M, 1) * sigma2\n",
    "X = np.concatenate((x1, x2), axis=1)\n",
    "```\n",
    "После чего сделайте матрицу поворота на угол $\\alpha$ и с помощью неё поверните набор точек в пространстве.  \n",
    "Посчитайте матрицу ковариации для сгенерированных точек.  \n",
    "Нарисуйте полученное облако точек и сравните его с облаком точек, полученных с помощью готовой функции:  `np.random.multivariate_normal()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae90cd",
   "metadata": {},
   "source": [
    "### 2. Плотность вероятности гауссового распределения\n",
    "В общем виде $n$-мерное гауссово распределение имеет вид:  \n",
    "$$p(x) = \\frac{1}{(\\sqrt{2\\pi})^n \\sqrt{detC}} e^{-\\frac{1}{2}(x-\\mu)^TС^{-1}(x-\\mu)} \\tag{1}$$  \n",
    "где $x\\in{R^n}$, $\\mu\\in{R^n}$ - среднее значение, $C$ - матрица ковариации.  \n",
    "\n",
    "Сгенерируйте набор точек в двумерном пространстве (как делали в пункте 1 или другим способом). Посчитайте для них среднее значение и матрицу ковариации $\\mu$ и $C$. Для оценки плотности вероятности (probability density function - `pdf`) гауссового распределения в произвольной точке используйте `scipy.stats.multivariate_normal` \n",
    "(https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html), либо можете посчитать вручную.  \n",
    "\n",
    "Визуализируйте набор точек и плотность вероятности, для чего изобразите точки одним цветом, а фон раскрасьте в соответсвии с плотностью вероятности, для чего можете использовать пример ниже:  \n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "yy = np.linspace(-1, 1, 100)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "pp = np.stack((XX.flatten(), YY.flatten()), axis=1)\n",
    "m = multivariate_normal(mean=[0,0], cov=[[0.3, 0], [0, 0.1]])\n",
    "ZZ = m.pdf(pp).reshape(XX.shape)\n",
    "\n",
    "IMG = plt.pcolor(XX, YY, ZZ)\n",
    "plt.colorbar(IMG)\n",
    "\n",
    "CS = plt.contour(XX, YY, m.pdf(pp).reshape(XX.shape), levels=[0.2, 0.4, 0.6, 0.8, 0.9], colors='k')\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d797c8",
   "metadata": {},
   "source": [
    "### 3. Бинарная классификация\n",
    "\n",
    "**Задание** \n",
    "Создайте случайный датасет, состоящий из двух классов $y\\in{\\{0, 1\\}}$, а признаки имеют две координаты. Посчитайте для каждого класса среднее значение и матрицу ковариации $\\mu_0, \\mu_1$ и $C_0, C_1$.\n",
    "\n",
    "Решение задачи классификации можно свести к оценке максимальной апостериорной вероятности (Maximum a posterior - MAP):\n",
    "$$y = \\underset{y\\in{0,1}}{argmax} p(y|x) \\tag{2}$$  \n",
    "которая в свою очередь через теорему Байеса равна:  \n",
    "$$p(y|x) = \\frac{p(x|y)p(y)}{p(x)} \\tag{3}$$\n",
    "В последней формуле вероятность p(y) называется априорной и не зависит от $x$ - оценить её можно просто как долю точек датасета принадлежащего заданному классу $y$. Вероятность p(x|y) называют правдоподобием (likelyhood). Обычно правдоподобие моделируется некоторой генерирующей моделью, например по формуле гаусса (1), и говорит насколько правдоподобно что точка с координатами $x$ могла быть \"сгенерирована\" в классе $y$. Знаменатель $p(x)$ в последней формуле не зависит от $y$, поэтому на него внимание не обращаем.  \n",
    "\n",
    "Таким образом, для бинарной классификации, т.е. для оценки принадлежности некоторой точки $x$ к классу 0 или 1, нужно сравнить две величины:  \n",
    "$$p(x|y=0)p(y=0) \\overset{?}{<>} p(x|y=1)p(y=1) \\tag{4}$$\n",
    "\n",
    "**Задание**\n",
    "Визуализируейте на одном рисунке датасет с точками обоих классов разными цветами. Раскрасьте фон изображения в соответствии с разностью левой и правой частей уравнения (4). Постройте на этом же рисунке разделяющую кривую, т.е. кривую соответствующую нулевой разнице между левой и правой частями. Для её построения используйте функции `plt.contour` с указанием конкретного уровня."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d38753",
   "metadata": {},
   "source": [
    "### 4. LDA (linear discriminant analysis)\n",
    "\n",
    "**Задание** Рассмотрите частный случай, когда для точек двух классов матрицы ковариации совпадают, т.е. $C_0=C_1$, но средние значения отличаются $\\mu_0\\neq\\mu_1$. Выпишите явное выражение для разделяющей поверхности в этом случае. Визуализируйте такой случай аналогично предыдущему пункту. Про линейный и квадратичный дискриминантный анализ можете почитать здесь (https://scikit-learn.org/stable/modules/lda_qda.html) или в учебнике Яндекса по ML (https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii)\n",
    "\n",
    "Реализуйте классификатор на основе метода линейного дискриминантного анализа по шаблону ниже. Данный алгоритм предполагает одинаковость матриц ковариации (т.е. считает одну общую матрицу ковариации, но разные средние) и использует линейное правило классификации. Желательно сделать реализацию, которая подходит не только для двумерного, но и для более общего случая.\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class myLDA(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ad2b7",
   "metadata": {},
   "source": [
    "### 5. Наивный байесовый классификатор (NaiveBayes)\n",
    "\n",
    "**Задание** \n",
    "Прочитать по наивный байесовый подход можно здесь (https://scikit-learn.org/stable/modules/naive_bayes.html) или в учебнике Яндекса по ML (https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii).  \n",
    "\n",
    "Суть наивного предположения состоит в том, чтобы не считать совместную плотность вероятности для $n$-мерного случая, а рассматривать каждую переменную как независимую и считать одномерные распределения:  \n",
    "$$p(x_1,x_2,...,x_n|y) => \\prod\\limits_{i=1}^{n}p(x_i|y)$$  \n",
    "Это можно также рассматривать как обнуление всех внедиагональных элементов матрицы ковариации.  \n",
    "\n",
    "**Задание** \n",
    "Реализуйте классификатор на основе наивного байесового подхода с одномерным гауссовым распределением для каждой переменной. Желательно сделать реализацию, которая подходит не только для двумерного, но и для более общего случая.\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class myNB(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca49be",
   "metadata": {},
   "source": [
    "### 6. Сравнение двух алгоритмов: LDA и NB\n",
    "\n",
    "Создайте датасет для бинарной классификации, либо вручную либо используйте функцию `sklearn.datasets.make_classification`\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)  \n",
    "\n",
    "Разделите датасет на обучающую и тестовую подвыборки.  \n",
    "Проверьте работоспособность разработанных вами двух алгоритмов.  \n",
    "Посчитайте основные метрики классификации: долю правильных ответов (accuracy), точность (Precision) и полноту (Recall).  \n",
    "(про метрики прочитайте здесь https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii)  \n",
    "Сделайте выводы.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbfa94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
